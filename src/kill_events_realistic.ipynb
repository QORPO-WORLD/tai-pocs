{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import time\n",
    "import tqdm\n",
    "from collections import deque\n",
    "\n",
    "from application.aws import AwsAPI\n",
    "from application.game import Game\n",
    "from application.utils import get_kill_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Realistic kill events test\n",
    "\n",
    "For this test, kill events were generated as close to real ones as possible: a tree structure was generated to map events to users, each user got 0 to 6 kills randomly assigned, tree was traversed from leaves to root, so that in the end only 1 user is alive. Context window size was set to 20 messages for performance reasons, so the model generates responses based on 9 previous events and the current event. Claude v2 was used, this notebook can be utilized to test other models' generations quality on that task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "kills = get_kill_events(\"data/kills.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_prompts = [\n",
    "    (\n",
    "        \"Imagine you're commenting the battle royale game match. You'll be getting kill events from the game like this one: \"\n",
    "        '[{\"victim\": {\"username\": str},\"KillInstigator\": {\"username\": str,\"Distance\": float,\"first_kill\": bool,\"used_weapon\": {\"type\": str,\"name\": str},\"Headshot\": bool,\"OneShot\": bool,\"num_kills\": int,\"previous_victims\": [str]},\"location\": str,\"num_players_alive\": int}]. '\n",
    "        \"Sometimes you will be getting more than one event in this list. \"\n",
    "        \"You'll need to comment on the kill events, using 3 senteces max. \"\n",
    "        \"If there are multiple events, you can decide either to comment on all of them or comment the last of them while also keeping in mind other events. \"\n",
    "        \"Even if there are multiple events, you should still be able to say everything in 3 sentences. \"\n",
    "        \"You're not supposed to always use each field for the comment, but you can use them if you think they're relevant. \"\n",
    "        \"Try to also remember previous events and if you see some patterns feel free to voice them. Understood?\"\n",
    "    ),\n",
    "    \"Understood. I'm ready to commentate on the battle royale game events.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from application.game import Player\n",
    "\n",
    "\n",
    "def _dump_messages_to_file(q, filename):\n",
    "    with open(filename, \"a\") as f:\n",
    "        f.write(f\"User: {q[-2]}\\n\")\n",
    "        f.write(f\"Assistant: {q[-1]}\\n\\n\")\n",
    "\n",
    "\n",
    "def _create_event_lists(game: Game, root: Player):\n",
    "    kill_events: list[list[dict]] = []\n",
    "    kill_events_flat = [kill_event for kill_event in game.get_kill_event(root)]\n",
    "    i = 0\n",
    "    while i < len(kill_events_flat):\n",
    "        kill_events.append([])\n",
    "        events_num = min(random.randint(1, 4), len(kill_events_flat) - i)\n",
    "        for _ in range(events_num):\n",
    "            kill_events[-1].append(kill_events_flat[i])\n",
    "            i += 1\n",
    "    return kill_events\n",
    "\n",
    "\n",
    "def _trim_queue(q: deque, context_window_size: int):\n",
    "    if len(q) + 2 > context_window_size:\n",
    "        for _ in range(len(init_prompts)):\n",
    "            q.popleft()\n",
    "        q.popleft()\n",
    "        q.popleft()\n",
    "        for prompt in reversed(init_prompts):\n",
    "            q.appendleft(prompt)\n",
    "\n",
    "\n",
    "def _get_sentences(q, aws: AwsAPI, model_id, temperature):\n",
    "    roles = (\"user\", \"assistant\")\n",
    "    model = aws.models_mapping[model_id]\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": roles[i % 2],\n",
    "            \"content\": [model.get_content(prompt)],\n",
    "        }\n",
    "        for i, prompt in enumerate(q)\n",
    "    ]\n",
    "    yield from aws.get_streamed_response(model_id, messages, time.time(), temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Nova Pro...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [01:35<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. time per event: 1.84s\n",
      "Testing Claude V2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [04:39<00:00,  5.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. time per event: 5.17s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def run_dialogue(model_id, region, context_window_size=20, temperature=0.9):\n",
    "    aws = AwsAPI(region)\n",
    "    game = Game(kills)\n",
    "    root = game.create_players_tree()\n",
    "    q = deque(init_prompts)\n",
    "    filename = f\"output/text/{int(time.time())}-{model_id}.txt\"\n",
    "\n",
    "    _dump_messages_to_file(q, filename)\n",
    "    kill_events = _create_event_lists(game, root)\n",
    "\n",
    "    elapsed_total = 0\n",
    "    for kill_events_list in tqdm.tqdm(kill_events):\n",
    "        _trim_queue(q, context_window_size)\n",
    "        q.append(json.dumps(kill_events_list))\n",
    "\n",
    "        start_time = time.time()\n",
    "        response = []\n",
    "        try:\n",
    "            for sentence in _get_sentences(q, aws, model_id, temperature):\n",
    "                response.append(sentence)\n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "        elapsed_total += time.time() - start_time\n",
    "\n",
    "        q.append(\"\".join(response))\n",
    "        _dump_messages_to_file(q, filename)\n",
    "    \n",
    "    print(f\"Avg. time per event: {elapsed_total / len(kill_events):.2f}s\")\n",
    "    # print(aws.get_bedrock_stats(model_id))\n",
    "\n",
    "print(\"Testing Nova Pro...\")\n",
    "run_dialogue(\"amazon.nova-pro-v1:0\", \"us-east-1\")\n",
    "print(\"Testing Claude V2...\")\n",
    "run_dialogue(\"anthropic.claude-v2\", \"eu-central-1\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
