{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "import time\n",
    "import tqdm\n",
    "from collections import deque\n",
    "\n",
    "from application.models import ModelID\n",
    "from application.aws import AwsAPI\n",
    "from application.game import Game\n",
    "from application.utils import (\n",
    "    get_game_events,\n",
    "    dump_message_to_file,\n",
    "    generate_batch_sizes,\n",
    "    trim_queue,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Realistic game events test\n",
    "\n",
    "For this test, game events were generated as close to real ones as possible: a tree structure was generated to map events to users, each user got 0 to 6 kills randomly assigned, tree was traversed from leaves to root, so that in the end only 1 user is alive. Context window size was set to 20 messages for performance reasons, so the model generates responses based on 9 previous events and the current event. Claude v2 was used, this notebook can be utilized to test other models' generations quality on that task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_events = get_game_events(\"data/events.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_prompts = [\n",
    "    (\n",
    "        \"Imagine you're commenting the battle royale game match. You'll be getting kill events from the game like this one: \"\n",
    "        '[{\"victim\": {\"username\": str},\"KillInstigator\": {\"username\": str,\"Distance\": float,\"first_kill\": bool,\"used_weapon\": {\"type\": str,\"name\": str},\"Headshot\": bool,\"OneShot\": bool,\"num_kills\": int,\"previous_victims\": [str]},\"location\": str,\"num_players_alive\": int}]. '\n",
    "        \"Sometimes you will be getting more than one event in this list. \"\n",
    "        \"You'll need to comment on the kill events, using 3 senteces max. \"\n",
    "        \"If there are multiple events, you can decide either to comment on all of them or comment the last of them while also keeping in mind other events. \"\n",
    "        \"Even if there are multiple events, you should still be able to say everything in 3 sentences. \"\n",
    "        \"You're not supposed to always use each field for the comment, but you can use them if you think they're relevant. \"\n",
    "        \"Try to also remember previous events and if you see some patterns feel free to voice them. Understood?\"\n",
    "    ),\n",
    "    \"Understood. I'm ready to commentate on the battle royale game events.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_sentences(q, aws: AwsAPI, model_id: ModelID, temperature: float):\n",
    "    roles = (\"user\", \"assistant\")\n",
    "    model = aws.models_mapping[model_id]\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": roles[i % 2],\n",
    "            \"content\": [model.get_content(prompt)],\n",
    "        }\n",
    "        for i, prompt in enumerate(q)\n",
    "    ]\n",
    "    yield from aws.get_streamed_response(model_id, messages, time.time(), temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Claude V3.5 Sonnet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [03:35<00:00,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. time per event: 3.71s\n",
      "Testing Nova Pro...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [01:36<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. time per event: 1.66s\n",
      "Testing Claude V2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [03:42<00:00,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. time per event: 3.83s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "game = Game(game_events)\n",
    "winner = game.create_players_tree()\n",
    "num_events_total = game.num_players_total - 1\n",
    "batch_sizes = generate_batch_sizes(num_events_total)\n",
    "\n",
    "\n",
    "def run_dialogue(model_id, region, context_window_size=20, temperature=0.9):\n",
    "    aws = AwsAPI(region)\n",
    "    q = deque(init_prompts)\n",
    "    events_nums = copy.copy(batch_sizes)\n",
    "    filename = f\"output/text/{int(time.time())}-{model_id.value}.txt\"\n",
    "\n",
    "    dump_message_to_file(\"User\", q[-2], filename)\n",
    "    dump_message_to_file(\"Assistant\", q[-1], filename)\n",
    "    game.start()\n",
    "\n",
    "    elapsed_total = 0\n",
    "    events_num = events_nums.pop()\n",
    "    game_events_list = []\n",
    "    for event in tqdm.tqdm(game.get_events(winner), total=num_events_total):\n",
    "        if events_num > 0:\n",
    "            game_events_list.append(event)\n",
    "            events_num -= 1\n",
    "            continue\n",
    "        else:\n",
    "            events_num = events_nums.pop()\n",
    "            game_events_list = [event]\n",
    "            events_num -= 1\n",
    "\n",
    "        trim_queue(q, context_window_size, init_prompts)\n",
    "\n",
    "        q.append(json.dumps(game_events_list))\n",
    "        dump_message_to_file(\"User\", q[-1], filename)\n",
    "\n",
    "        start_time = time.time()\n",
    "        response = []\n",
    "        try:\n",
    "            for sentence in _get_sentences(q, aws, model_id, temperature):\n",
    "                response.append(sentence)\n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "        elapsed_total += time.time() - start_time\n",
    "\n",
    "        q.append(\"\".join(response))\n",
    "        dump_message_to_file(\"Assistant\", q[-1], filename)\n",
    "\n",
    "    print(f\"Avg. time per event: {elapsed_total / len(batch_sizes):.2f}s\")\n",
    "    # print(aws.get_bedrock_stats(model_id))\n",
    "\n",
    "\n",
    "print(\"Testing Claude V3.5 Sonnet...\")\n",
    "run_dialogue(ModelID.CLAUDE_SONNET, \"eu-central-1\")\n",
    "print(\"Testing Nova Pro...\")\n",
    "run_dialogue(ModelID.NOVA_PRO, \"us-east-1\")\n",
    "print(\"Testing Claude V2...\")\n",
    "run_dialogue(ModelID.CLAUDE_V2, \"eu-central-1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
